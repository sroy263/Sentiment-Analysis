{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Desktop\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\Desktop\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import re\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "#import tqdm as tqdm\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words(\"english\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "english_stemmer=nltk.stem.SnowballStemmer('english')\n",
    "import nltk\n",
    "lemm=nltk.wordnet.WordNetLemmatizer()\n",
    "\n",
    "from gensim import summarization\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from math import floor,ceil\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import LSTM, Dense, Embedding\n",
    "# import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('new.csv',encoding= \"ISO-8859-13\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Author', 'Category', 'Content', 'Date', 'Links',\n",
       "       'Source', 'Subtitle', 'Tags', 'Title'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning( review, remove_stopwords=True):\n",
    "    review_text = re.sub(\"[^a-zA-Z0-9]\",\" \", review)\n",
    "    words = review_text.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "\n",
    "    b=[]\n",
    "    stemmer = english_stemmer \n",
    "    for word in words:\n",
    "        b.append(lemm.lemmatize(word))\n",
    "# #     for word in words:\n",
    "# #         b.append(stemmer.stem(word))\n",
    "    return(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "content = []\n",
    "title =[]\n",
    "for review in data['Content']:\n",
    "    content.append( \" \".join(cleaning(str(review))))\n",
    "for t in data['Title']:\n",
    "    title.append(\" \".join(cleaning(str(t))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# vectorizer = TfidfVectorizer(min_df=4, max_features = 10000)\n",
    "# vz = vectorizer.fit_transform(content)\n",
    "# tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Desktop\\Anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import csv\n",
    "file = open('C:\\\\Users\\\\Desktop\\\\Desktop\\\\roy\\\\sentiment_2.csv','w',encoding='ISO-8859-13')\n",
    "file.write(\"Title,Content,Sentiment\\n\")\n",
    "Senti = SentimentIntensityAnalyzer()\n",
    "c = content\n",
    "t=title\n",
    "# for sentence in t:\n",
    "#     ss=Senti.polarity_scores(sentence)\n",
    "#     file.write(sentence+\",\"+str(ss['compound'])+\"\\n\")\n",
    "for sentence,titl in zip(c,t):\n",
    "    ss = Senti.polarity_scores(sentence)\n",
    "    file.write(titl+\",\"+sentence+\",\"+str(ss['compound'])+\"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
